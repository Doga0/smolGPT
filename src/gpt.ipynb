{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vrl7JuBzd3Qg",
        "outputId": "eed40957-b4c8-45c9-fc17-ca787e8f6763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "Di8SUONb7O0K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "Wyr5mLasNM-C"
      },
      "outputs": [],
      "source": [
        "class GPT_CONFIG:\n",
        "  block_size:     int = 256\n",
        "  vocab_size:     int = 50257\n",
        "  embed_dim:      int = 768\n",
        "  num_heads:      int = 12\n",
        "  num_layers:     int = 12\n",
        "  dropout:        float = 0.1\n",
        "  bias:           bool = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "ahOV4V6t8DWf"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(cfg.embed_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(cfg.embed_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    std = x.std(dim=-1, keepdim=True)\n",
        "    norm_x = (x - mean) / (std + self.eps)\n",
        "    return self.scale * norm_x + self.shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "RCK0jZ1G_g0f"
      },
      "outputs": [],
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) *\n",
        "     (x + 0.044715 * torch.pow(x, 3))\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "jjMgJwKz8Grf"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear_1 = nn.Linear(cfg.embed_dim, cfg.embed_dim * 4, bias=cfg.bias)\n",
        "    self.gelu = GELU()\n",
        "    self.linear_2 = nn.Linear(cfg.embed_dim * 4, cfg.embed_dim, bias=cfg.bias)\n",
        "    self.dropout = nn.Dropout(cfg.dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear_2(self.dropout(self.gelu(self.linear_1(x))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "oEZFsgTvB1-d"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_dim = cfg.embed_dim\n",
        "    self.h = cfg.num_heads\n",
        "    assert cfg.embed_dim % cfg.num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "    self.d_k = cfg.embed_dim // cfg.num_heads # 512 / 8 = 64 by default\n",
        "    self.w_q = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
        "    self.w_k = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
        "    self.w_v = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
        "\n",
        "    # fully connected layer: 8*64x512 or 512x512\n",
        "    self.w_o = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(cfg.dropout)\n",
        "    self.register_buffer(\"mask\", torch.triu(torch.ones(cfg.block_size, cfg.block_size), diagonal=1))\n",
        "\n",
        "  def ScaledDotProductAttention(self, query, key, value, mask, dropout):\n",
        "    d_k = query.shape[-1]\n",
        "\n",
        "    # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "    attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "    attention_scores = torch.softmax(attention_scores, dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores = self.dropout(attention_scores) # (batch, h, seq_len, seq_len)\n",
        "\n",
        "    return torch.matmul(attention_scores, value), attention_scores\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch, num_tokens, d_in = x.shape\n",
        "\n",
        "    query = self.w_q(x) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "    key = self.w_k(x) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "    value = self.w_v(x) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "\n",
        "    # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "    query = query.view(batch, num_tokens, self.h, self.d_k).transpose(1, 2)\n",
        "    key = key.view(batch, num_tokens, self.h, self.d_k).transpose(1, 2)\n",
        "    value = value.view(batch, num_tokens, self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "    mask = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "    x, self.attention_scores = self.ScaledDotProductAttention(query, key, value, mask, self.dropout)\n",
        "\n",
        "    # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "    return self.w_o(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "THfk1Q8NMqwa"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.norm_1 = LayerNorm(cfg)\n",
        "    self.attn = MultiHeadAttention(cfg)\n",
        "    self.dropout = nn.Dropout(cfg.dropout)\n",
        "    self.norm_2 = LayerNorm(cfg)\n",
        "    self.ff = FeedForward(cfg)\n",
        "\n",
        "  def forward(self, x):\n",
        "    shortcut = x\n",
        "    x = self.norm_1(x)\n",
        "    x = self.attn(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    shortcut = x\n",
        "    x = self.norm_2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "RKIZd7NgR7zr"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.tok_embed = nn.Embedding(cfg.vocab_size, cfg.embed_dim)\n",
        "    self.pos_embed = nn.Embedding(cfg.block_size, cfg.embed_dim)\n",
        "    self.dropout = nn.Dropout(cfg.dropout)\n",
        "\n",
        "    self.blocks = nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range(cfg.num_layers)]\n",
        "    )\n",
        "\n",
        "    self.norm = LayerNorm(cfg)\n",
        "    self.linear = nn.Linear(cfg.embed_dim, cfg.vocab_size)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    batch_size, seq_len = idx.shape\n",
        "\n",
        "    tok_embed = self.tok_embed(idx)\n",
        "    pos_embed = self.pos_embed(torch.arange(seq_len, device=idx.device))\n",
        "    x = self.dropout(tok_embed + pos_embed)\n",
        "    x = self.blocks(x)\n",
        "    x = self.norm(x)\n",
        "    x = self.linear(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPTDataset:\n",
        "  def __init__(self, text, tokenizer, max_len, stride):\n",
        "    super().__init__()\n",
        "\n",
        "    input_ids = []\n",
        "    target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(\"gpt-3.5-turbo\", allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "    for i in range(0, len(token_ids) - max_len, stride):\n",
        "      input_chunk = token_ids[i: i + max_len]\n",
        "      target_chunk = token_ids[i + 1: i + max_len + 1]\n",
        "\n",
        "      input_ids.append(input_chunk)\n",
        "      target_ids.append(target_chunk)\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      return {\n",
        "          \"input_ids\": self.input_ids[idx],\n",
        "          \"target_ids\": self.target_ids[idx]\n",
        "      }\n",
        "\n",
        "def dataloader(text, batch_size=4, max_len=256,\n",
        "              stride=128, shuffle=True, drop_last=True,\n",
        "              num_workers=0):\n",
        "  \n",
        "  tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "  dataset = GPTDataset(text, tokenizer, max_len, stride)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "                          drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "  return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = GPT_CONFIG()\n",
        "model = GPT(config)\n",
        "model.to(device)\n",
        "\n",
        "dataset = load_dataset(\"Mursel/Turkish-wikipedia-10k\")\n",
        "\n",
        "split_dataset = dataset[\"train\"].train_test_split(test_size=0.2)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "val_dataset = split_dataset[\"test\"]\n",
        "\n",
        "text = val_dataset[0][\"content\"]\n",
        "\n",
        "train_ratio = 0.8\n",
        "\n",
        "train_size = int(len(text)) * train_ratio\n",
        "\n",
        "train_dataset = text[:train_size]\n",
        "val_dataset = text[train_size:]\n",
        "\n",
        "train_loader = dataloader(train_dataset, 2, cfg.block_size, cfg.block_size, \n",
        "                          drop_last=False, shuffle=False, num_workers=0)\n",
        "\n",
        "val_loader = dataloader(val_dataset, 2, cfg.block_size, cfg.block_size, \n",
        "                        drop_last=False, shuffle=False, num_workers=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "-VMNZsnhWvsK"
      },
      "outputs": [],
      "source": [
        "class Generate:\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def greedy_decode(self, model, idx, max_new_tokens, block_size):\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "\n",
        "      with torch.no_grad():\n",
        "        logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "      probs = torch.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "  def beam_decode(self):\n",
        "    pass\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
