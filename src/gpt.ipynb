{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vrl7JuBzd3Qg",
        "outputId": "eed40957-b4c8-45c9-fc17-ca787e8f6763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "Di8SUONb7O0K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "Wyr5mLasNM-C"
      },
      "outputs": [],
      "source": [
        "class GPT_CONFIG:\n",
        "  block_size:     int = 1024\n",
        "  vocab_size:     int = 50257\n",
        "  embed_dim:      int = 768\n",
        "  num_heads:      int = 12\n",
        "  num_layers:     int = 12\n",
        "  dropout:        float = 0.1\n",
        "  bias:           bool = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "ahOV4V6t8DWf"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(cfg.embed_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(cfg.embed_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    std = x.std(dim=-1, keepdim=True)\n",
        "    norm_x = (x - mean) / (std + self.eps)\n",
        "    return self.scale * norm_x + self.shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "RCK0jZ1G_g0f"
      },
      "outputs": [],
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) *\n",
        "     (x + 0.044715 * torch.pow(x, 3))\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "jjMgJwKz8Grf"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear_1 = nn.Linear(cfg.embed_dim, cfg.embed_dim * 4, bias=cfg.bias)\n",
        "    self.gelu = GELU()\n",
        "    self.linear_2 = nn.Linear(cfg.embed_dim * 4, cfg.embed_dim, bias=cfg.bias)\n",
        "    self.dropout = nn.Dropout(cfg.dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear_2(self.dropout(self.gelu(self.linear_1(x))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "oEZFsgTvB1-d"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_dim = cfg.embed_dim\n",
        "    self.h = cfg.num_heads\n",
        "    assert cfg.embed_dim % cfg.num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "    self.d_k = cfg.embed_dim // cfg.num_heads # 512 / 8 = 64 by default\n",
        "    self.w_q = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
        "    self.w_k = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
        "    self.w_v = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
        "\n",
        "    # fully connected layer: 8*64x512 or 512x512\n",
        "    self.w_o = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(cfg.dropout)\n",
        "    self.register_buffer(\"mask\", torch.triu(torch.ones(cfg.block_size, cfg.block_size), diagonal=1))\n",
        "\n",
        "  def ScaledDotProductAttention(self, query, key, value, mask, dropout):\n",
        "    d_k = query.shape[-1]\n",
        "\n",
        "    # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "    attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "    attention_scores = torch.softmax(attention_scores, dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores = self.dropout(attention_scores) # (batch, h, seq_len, seq_len)\n",
        "\n",
        "    return torch.matmul(attention_scores, value), attention_scores\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch, num_tokens, d_in = x.shape\n",
        "\n",
        "    query = self.w_q(x) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "    key = self.w_k(x) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "    value = self.w_v(x) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "\n",
        "    # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "    query = query.view(batch, num_tokens, self.h, self.d_k).transpose(1, 2)\n",
        "    key = key.view(batch, num_tokens, self.h, self.d_k).transpose(1, 2)\n",
        "    value = value.view(batch, num_tokens, self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "    mask = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "    x, self.attention_scores = self.ScaledDotProductAttention(query, key, value, mask, self.dropout)\n",
        "\n",
        "    # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "    return self.w_o(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "THfk1Q8NMqwa"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.norm_1 = LayerNorm(cfg)\n",
        "    self.attn = MultiHeadAttention(cfg)\n",
        "    self.dropout = nn.Dropout(cfg.dropout)\n",
        "    self.norm_2 = LayerNorm(cfg)\n",
        "    self.ff = FeedForward(cfg)\n",
        "\n",
        "  def forward(self, x):\n",
        "    shortcut = x\n",
        "    x = self.norm_1(x)\n",
        "    x = self.attn(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    shortcut = x\n",
        "    x = self.norm_2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "RKIZd7NgR7zr"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.tok_embed = nn.Embedding(cfg.vocab_size, cfg.embed_dim)\n",
        "    self.pos_embed = nn.Embedding(cfg.block_size, cfg.embed_dim)\n",
        "    self.dropout = nn.Dropout(cfg.dropout)\n",
        "\n",
        "    self.blocks = nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range(cfg.num_layers)]\n",
        "    )\n",
        "\n",
        "    self.norm = LayerNorm(cfg)\n",
        "    self.linear = nn.Linear(cfg.embed_dim, cfg.vocab_size)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    batch_size, seq_len = idx.shape\n",
        "\n",
        "    tok_embed = self.tok_embed(idx)\n",
        "    pos_embed = self.pos_embed(torch.arange(seq_len, device=idx.device))\n",
        "    x = self.dropout(tok_embed + pos_embed)\n",
        "    x = self.blocks(x)\n",
        "    x = self.norm(x)\n",
        "    x = self.linear(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "-VMNZsnhWvsK"
      },
      "outputs": [],
      "source": [
        "class Generate:\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def greedy_decode(self, model, idx, max_new_tokens, block_size):\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "\n",
        "      with torch.no_grad():\n",
        "        logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "      probs = torch.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "  def beam_decode(self):\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "collapsed": true,
        "id": "LOf1q3MYcQCP"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "encodings_ = 'cl100k_base'\n",
        "model_ = 'gpt-3.5-turbo'\n",
        "\n",
        "class Tokenizer:\n",
        "  def __init__(self, encoding=None, model=None):\n",
        "    self.encodings = encoding if encoding is not None else encodings_\n",
        "    self.model = model if model is not None else model_\n",
        "    self.tokenizer = tiktoken.get_encoding(self.encodings)\n",
        "    self.tokenizer = tiktoken.encoding_for_model(self.model)\n",
        "\n",
        "  def encode(self, text):\n",
        "    return self.tokenizer.encode(text)\n",
        "\n",
        "  def decode(self, ids):\n",
        "    return self.tokenizer.decode(ids)\n",
        "\n",
        "  def get_vocab(self):\n",
        "    return self.tokenizer.n_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBULPK4yd7qz",
        "outputId": "38ebffd9-36f7-43fe-94db-058d664bc863"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:  tensor([[12805,  5304,   264,   892, 49969, 31716, 32255, 36861, 18583, 31127,\n",
            "         23092, 41584, 12370, 27316]])\n",
            "Output length:  14\n"
          ]
        }
      ],
      "source": [
        "context = \"Once upon a time\"\n",
        "tokenizer = Tokenizer()\n",
        "enc = tokenizer.encode(context)\n",
        "enc_tensor = torch.tensor(enc).unsqueeze(0)\n",
        "\n",
        "config = GPT_CONFIG()\n",
        "model = GPT(config)\n",
        "model.eval()\n",
        "\n",
        "generate = Generate()\n",
        "output = generate.greedy_decode(model, enc_tensor, 10, config.block_size)\n",
        "\n",
        "print(\"Output: \", output)\n",
        "print(\"Output length: \", len(output[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui3Rro4De6eb",
        "outputId": "f662da54-f5c3-4038-b3dd-918b14b6fcf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once upon a time energiesmissing Together attain forKey maxim_USE.volumeEIFandidates\n"
          ]
        }
      ],
      "source": [
        "decoded_text = tokenizer.decode(output.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
