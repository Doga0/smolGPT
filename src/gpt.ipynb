{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Di8SUONb7O0K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import sentencepiece as spm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLs3w4rzp8j9",
        "outputId": "31cfc6a7-4569-4fe7-f710-559684d310c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Number of gpus: 1\n"
          ]
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\" if torch.cuda.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "num_of_gpus = torch.cuda.device_count()\n",
        "print(\"Number of gpus:\", num_of_gpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWmNRv5XI2bW"
      },
      "outputs": [],
      "source": [
        "# config parameters\n",
        "dataset = load_dataset(\"Mursel/Turkish-wikipedia-10k\")\n",
        "\n",
        "epochs = 6\n",
        "batch_size = 2\n",
        "learning_rate = 6e-4\n",
        "min_lr = 6e-5\n",
        "\n",
        "betas = (0.9, 0.95)\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
        "weight_decay=0.1\n",
        "\n",
        "freq = 1\n",
        "\n",
        "class GPT_CONFIG:\n",
        "  block_size:     int = 256\n",
        "  vocab_size:     int = 32000\n",
        "  embed_dim:      int = 768\n",
        "  num_heads:      int = 12\n",
        "  num_layers:     int = 12\n",
        "  dropout:        float = 0.1\n",
        "  bias:           bool = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahOV4V6t8DWf"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(cfg.embed_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(cfg.embed_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    std = x.std(dim=-1, keepdim=True)\n",
        "    norm_x = (x - mean) / (std + self.eps)\n",
        "    return self.scale * norm_x + self.shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCK0jZ1G_g0f"
      },
      "outputs": [],
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2 / torch.pi)) *\n",
        "     (x + 0.044715 * torch.pow(x, 3))\n",
        "    ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjMgJwKz8Grf"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear_1 = nn.Linear(cfg.embed_dim, cfg.embed_dim * 4, bias=cfg.bias)\n",
        "    self.gelu = GELU()\n",
        "    self.linear_2 = nn.Linear(cfg.embed_dim * 4, cfg.embed_dim, bias=cfg.bias)\n",
        "    self.dropout = nn.Dropout(cfg.dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear_2(self.dropout(self.gelu(self.linear_1(x))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEZFsgTvB1-d"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embed_dim = cfg.embed_dim\n",
        "    self.h = cfg.num_heads\n",
        "    assert cfg.embed_dim % cfg.num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "\n",
        "    self.d_k = cfg.embed_dim // cfg.num_heads\n",
        "    self.w_q = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
        "    self.w_k = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
        "    self.w_v = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
        "\n",
        "    self.w_o = nn.Linear(cfg.embed_dim, cfg.embed_dim)\n",
        "\n",
        "    self.dropout = nn.Dropout(cfg.dropout)\n",
        "    self.register_buffer(\"mask\", torch.triu(torch.ones(cfg.block_size, cfg.block_size), diagonal=1))\n",
        "\n",
        "  def ScaledDotProductAttention(self, query, key, value, mask, dropout):\n",
        "    d_k = query.shape[-1]\n",
        "\n",
        "    # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)\n",
        "    attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "    attention_scores = torch.softmax(attention_scores, dim=-1)\n",
        "\n",
        "    if dropout is not None:\n",
        "      attention_scores = self.dropout(attention_scores) # (batch, h, seq_len, seq_len)\n",
        "\n",
        "    return torch.matmul(attention_scores, value), attention_scores\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch, num_tokens, d_in = x.shape\n",
        "\n",
        "    query = self.w_q(x) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "    key = self.w_k(x) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "    value = self.w_v(x) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "\n",
        "    # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "    query = query.view(batch, num_tokens, self.h, self.d_k).transpose(1, 2)\n",
        "    key = key.view(batch, num_tokens, self.h, self.d_k).transpose(1, 2)\n",
        "    value = value.view(batch, num_tokens, self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "    mask = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "    x, self.attention_scores = self.ScaledDotProductAttention(query, key, value, mask, self.dropout)\n",
        "\n",
        "    # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "    x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "    return self.w_o(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THfk1Q8NMqwa"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.norm_1 = LayerNorm(cfg)\n",
        "    self.attn = MultiHeadAttention(cfg)\n",
        "    self.dropout = nn.Dropout(cfg.dropout)\n",
        "    self.norm_2 = LayerNorm(cfg)\n",
        "    self.ff = FeedForward(cfg)\n",
        "\n",
        "  def forward(self, x):\n",
        "    shortcut = x\n",
        "    x = self.norm_1(x)\n",
        "    x = self.attn(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    shortcut = x\n",
        "    x = self.norm_2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.dropout(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKIZd7NgR7zr"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    self.tok_embed = nn.Embedding(cfg.vocab_size, cfg.embed_dim)\n",
        "    self.pos_embed = nn.Embedding(cfg.block_size, cfg.embed_dim)\n",
        "    self.dropout = nn.Dropout(cfg.dropout)\n",
        "\n",
        "    self.blocks = nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range(cfg.num_layers)]\n",
        "    )\n",
        "\n",
        "    self.norm = LayerNorm(cfg)\n",
        "    self.linear = nn.Linear(cfg.embed_dim, cfg.vocab_size)\n",
        "\n",
        "  def forward(self, idx):\n",
        "    batch_size, seq_len = idx.shape\n",
        "\n",
        "    tok_emb = self.tok_embed(idx)\n",
        "    pos_emb = self.pos_embed(torch.arange(seq_len, device=idx.device))\n",
        "    x = self.dropout(tok_emb + pos_emb)\n",
        "    x = self.blocks(x)\n",
        "    x = self.norm(x)\n",
        "    logits = self.linear(x)\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzeyEK0ygxF4"
      },
      "outputs": [],
      "source": [
        "class GPTDataset:\n",
        "  def __init__(self, text, tokenizer, max_len, stride):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "    for i in range(0, len(token_ids) - max_len, stride):\n",
        "      input_chunk = token_ids[i: i + max_len]\n",
        "      target_chunk = token_ids[i + 1: i + max_len + 1]\n",
        "\n",
        "      self.input_ids.append(input_chunk)\n",
        "      self.target_ids.append(target_chunk)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor(self.input_ids[idx]),\n",
        "        \"target_ids\": torch.tensor(self.target_ids[idx])\n",
        "    }\n",
        "\n",
        "def dataloader(tokenizer, text, batch_size, max_len,\n",
        "              stride, shuffle, drop_last,\n",
        "              num_workers=0):\n",
        "\n",
        "  dataset = GPTDataset(text, tokenizer, max_len, stride)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,\n",
        "                          drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "  return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "niF7fdrujO_o",
        "outputId": "37f840b8-75ad-446c-dea8-7835c0b44d68"
      },
      "outputs": [],
      "source": [
        "split_dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "val_dataset = split_dataset[\"test\"]\n",
        "\n",
        "train_text = \" \".join([entry[\"poem\"] for entry in train_dataset])\n",
        "val_text = \" \".join([entry[\"poem\"] for entry in val_dataset])\n",
        "\n",
        "text = train_text + val_text\n",
        "\n",
        "tokenizer = spm.SentencePieceProcessor(model_file=\"/vocab/tokenizer_32k.model\")\n",
        "\n",
        "print(\"Characters: \", len(text))\n",
        "print(\"Tokens: \", len(tokenizer.Encode(text)))\n",
        "\n",
        "config = GPT_CONFIG()\n",
        "\n",
        "print(\"n_vocab: \",tokenizer.n_vocab)\n",
        "\n",
        "train_ratio = 0.75\n",
        "\n",
        "train_size = int(len(text) * train_ratio)\n",
        "\n",
        "train_dataset = text[:train_size]\n",
        "val_dataset = text[train_size:]\n",
        "\n",
        "print(\"Train size: \", len(train_dataset))\n",
        "print(\"Val size: \", len(val_dataset))\n",
        "\n",
        "train_loader = dataloader(tokenizer, train_dataset, batch_size, config.block_size, config.block_size,\n",
        "                          drop_last=False, shuffle=False)\n",
        "\n",
        "val_loader = dataloader(tokenizer, val_dataset, batch_size, config.block_size, config.block_size,\n",
        "                        drop_last=False, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSgVNmXp78zu"
      },
      "outputs": [],
      "source": [
        "total_steps = int(epochs * len(train_loader) / batch_size)\n",
        "warmup_steps = int(0.1 * total_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "EF59cJDRDaLz",
        "outputId": "e956a41c-e1c4-4836-b59c-fdd6e4bb65e7"
      },
      "outputs": [],
      "source": [
        "for batch in train_loader:\n",
        "  x = batch['input_ids'].to(device)\n",
        "  y = batch['target_ids'].to(device)\n",
        "  print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6oQP4Ql4Db0L",
        "outputId": "8838623d-487f-4ef9-a469-d32dcb82a9bc"
      },
      "outputs": [],
      "source": [
        "for batch in val_loader:\n",
        "  x = batch['input_ids'].to(device)\n",
        "  y = batch['target_ids'].to(device)\n",
        "  print(x.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "bMQBUnxSBZEE",
        "outputId": "f5efa1a9-613e-4605-88e2-b996d4b69726"
      },
      "outputs": [],
      "source": [
        "model = GPT(config)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmKcU122tldw"
      },
      "outputs": [],
      "source": [
        "# learning rate warmup with cosine decay\n",
        "def lr_scheduler(step):\n",
        "\n",
        "  if total_steps < warmup_steps:\n",
        "    raise ValueError(\"Total steps must be greater or equal to warm up steps.\")\n",
        "\n",
        "  if step < warmup_steps:\n",
        "    return learning_rate * step / warmup_steps\n",
        "\n",
        "  if step > total_steps:\n",
        "    return min_lr\n",
        "\n",
        "  learning_rate = min_lr + 0.5 * (learning_rate - min_lr) *\n",
        "  (1 + torch.cos((torch.pi * step - warmup_steps) / float(total_steps- warmup_steps)))\n",
        "\n",
        "  return learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lM1M-243HbF"
      },
      "outputs": [],
      "source": [
        "\n",
        "optimizer = optim.AdamW(model.parameters(), learning_rate, betas, weight_decay)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "def train_one_epoch(model, train_loader, freq, epoch):\n",
        "\n",
        "  lr = lr_scheduler(epoch)\n",
        "\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "\n",
        "  loss_total = 0.\n",
        "  last_loss = 0.\n",
        "  size = len(train_loader)\n",
        "  step = 0\n",
        "\n",
        "  for i in train_loader:\n",
        "    inputs  = i['input_ids'].to(device)\n",
        "    targets = i['target_ids'].to(device)\n",
        "\n",
        "    if inputs.max() >= config.vocab_size or inputs.min() < 0:\n",
        "      raise ValueError(f\"Input ids out of range. Found min: {inputs.min().item()}, max: {inputs.max().item()}\")\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    outputs = outputs.view(-1, outputs.size(-1))\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    loss = loss_fn(outputs, targets)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_total += loss.item()\n",
        "    step += 1\n",
        "    if step % freq == 0:\n",
        "      last_loss = loss_total / freq\n",
        "      print(f\"  Batch: {step} | LR: {lr} | Loss: {last_loss}\")\n",
        "      loss_total = 0.\n",
        "\n",
        "  return last_loss\n",
        "\n",
        "def train(model, train_loader, val_loader, epochs, freq):\n",
        "\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    print(f\"\\nEPOCH {epoch + 1}\")\n",
        "    print(\"---------------------------------------------\")\n",
        "\n",
        "    avg_loss = train_one_epoch(freq, model, train_loader, epoch)\n",
        "\n",
        "    val_steps = 0\n",
        "    total_val_loss = 0.\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for i, val_data in enumerate(val_loader):\n",
        "        val_inputs, val_targets = val_data['input_ids'].to(device), val_data['target_ids'].to(device)\n",
        "\n",
        "        val_outputs = model(val_inputs)\n",
        "\n",
        "        val_outputs = val_outputs.view(-1, val_outputs.size(-1))\n",
        "        val_targets = val_targets.view(-1)\n",
        "\n",
        "        loss = loss_fn(val_outputs, val_targets)\n",
        "\n",
        "        total_val_loss += loss.item()\n",
        "        val_steps += 1\n",
        "\n",
        "    avg_val_loss = total_val_loss / val_steps\n",
        "    print(f\"LOSS Train: {avg_loss} Valid: {avg_val_loss}\")\n",
        "\n",
        "    train_losses.append(avg_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "  return train_losses, val_losses\n",
        "\n",
        "def plot_losses(epochs, train_losses, val_losses):\n",
        "  plt.figure()\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  ax.plot(epochs, train_losses, label=\"Train Loss\")\n",
        "  ax.plot(epochs, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "\n",
        "  ax.set_xlabel(\"Epochs\")\n",
        "  ax.set_ylabel(\"Loss\")\n",
        "  ax.legend(loc=\"upper right\")\n",
        "\n",
        "  fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQJ2pWkP0Im8"
      },
      "outputs": [],
      "source": [
        "train_losses, val_losses = train(model, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-4BX-nirqYy"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"/src/model.pth\")\n",
        "model = GPT(GPT_CONFIG)\n",
        "model.load_state_dict(torch.load(\"/src/model.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Inp7topXuBCX"
      },
      "outputs": [],
      "source": [
        "plot_losses(range(1, epochs + 1), train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR4RX19v35Og"
      },
      "outputs": [],
      "source": [
        "def visualize_attn():\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VMNZsnhWvsK"
      },
      "outputs": [],
      "source": [
        "class Generate:\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def greedy_decode(self, model, idx, max_new_tokens, block_size):\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "      idx_cond = idx[:, -block_size:]\n",
        "\n",
        "      with torch.no_grad():\n",
        "        logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "      probs = torch.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "      idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "  def beam_decode(self):\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2L9RXgJq0Wj"
      },
      "outputs": [],
      "source": [
        "context = \"Merhaba, ben\"\n",
        "enc = tokenizer.Encode(context)\n",
        "enc_tensor = torch.tensor(enc).unsqueeze(0)\n",
        "\n",
        "generate = Generate()\n",
        "output = generate.greedy_decode(model, enc_tensor, 3, config.block_size)\n",
        "\n",
        "print(\"Output: \", output)\n",
        "print(\"Output length: \", len(output[0]))\n",
        "\n",
        "decoded_text = tokenizer.Decode(output.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
